env.name=ide
cluster.name=${component.name}"-"${env.name}
instance.fqn=${cluster.name}
log.path.current=/tmp/logs/${instance.fqn}
log.path.archive=/tmp/logs/${instance.fqn}/archive
log.level=DEBUG
sysout.detach=false
syserr.detach=false
daemon.pidfile=${log.path.current}/${instance.fqn}.pid
port=9999
host=127.0.0.1
server.root =
{
  system{
    conf{
      spark {
        master="yarn"
        deploy-mode="cluster"
        app.name=${instance.fqn}
        spark.executor.memory="16g"
        spark.rpc.netty.dispatcher.numThreads="2"
        spark.logConf="true"
        spark.serializer=org.apache.spark.serializer.KryoSerializer
        log4j.rootCategory="WARN, console"
      }
      akka{
        actorSystem="pipelineSystem"
        actors{
          listener="ramanujan_pipeline_listener"
          master="ramanujan_pipeline_master"
          service="ramanujan_service_actor"
        }
      }
    }
  }
  sinks{
    kafka{
      #brokers="kafka01.production.askmebazaar.com:9092,kafka02.production.askmebazaar.com:9092,kafka03.production.askmebazaar.com:9092"
      bootstrap.servers="kafka01.production.askmebazaar.com:9092,kafka02.production.askmebazaar.com:9092,kafka03.production.askmebazaar.com:9092"
      metadata.brokers.list="kafka01.production.askmebazaar.com:2181,kafka02.production.askmebazaar.com:2181,kafka03.production.askmebazaar.com:2181"
      group.id="ramanujan"
      producer.type="async"
      key.serializer="org.apache.kafka.common.serialization.StringSerializer"
      value.serializer="org.apache.kafka.common.serialization.StringSerializer"
      auto.create.topics.enable="true"
      streaming.microbatch.time=10
      streaming.connection.timeout="1000"
    }
    hdfs{
      zookeeper.url="localhost:2181"
      prefixPath="hdfs://"
      alphanumeric="[^A-Za-z0-9]"
      tsPartitionEnd=10
      url="askmehadoop"
      port=9000
      nullUserType="none"
    }
    druid{
      overlord=overlord
      firehose="druid:firehose:%s"
      discovery="/druid/discovery"
    }
  }
  streaming{
    kafka{
      batchDuration=120
      cluster{
        brokers{
          staging="kafka01.staging.askmebazaar.com:9092,kafka02.staging.askmebazaar.com:9092,kafka03.staging.askmebazaar.com:9092"
          production="kafka01.production.askmebazaar.com:9092,kafka02.production.askmebazaar.com:9092,kafka03.production.askmebazaar.com:9092"
        }
        type{
          staging="staging"
          production="production"
        }
      }
    }
  }
  db{
    conn{
      jdbc=jdbc
      use=mysql
      hive{
        driver="org.apache.hive.jdbc.HiveDriver"
        version="hive2"
        host=localhost
        port=10000
        defaultdb="default"
      }
    }
    type{
        mysql=mysql
        postgres=postgresql
        sqlserver=sqlserver
    }
    internal{
      driver="com.mysql.jdbc.Driver"
      url="analytics.c0wj8qdslqom.ap-southeast-1.rds.amazonaws.com"
      user=root
      port=3306
      password="abcd1234"
      dbname=Ramanujan
      tables{
        runninglogs{
          name=RunningLOGS
          cols{
              id=id
              host=host
              port=port
              dbname=dbname
              dbtable=dbtable
              runTimeStamp=runTimeStamp
              hash=hash
              exceptions=exceptions
              notes=notes
          }
        }
        kafkaRunninglogs{
          name=kafkaRunningLOGS
          cols{
              id=id
              cluster=cluster
              topic=topic
              alias=alias
              groupName=groupName
              runTimeStamp=runTimeStamp
              hash=hash
              exceptions=exceptions
              notes=notes
          }
        }
        varTypeRecordsTable{
          name=VarTypeRecordsTable
          cols{
              tablename=tablename
              colname=colname
              coltype=coltype
              usertype=usertype
          }
        }
        kafkaVarTypeRecordsTable{
          name=kafkaVarTypeRecordsTable
          cols{
              topicname=topicname
              kafkaColname=kafkaColname
              kafkaUsertype=kafkaUsertype
          }
        }
        requests{
          name=Requests
          cols{
            request=request
            processDate=processDate
            host=host
            port=port
            dbname=dbname
            dbtable=dbtable
            lastStarted=lastStarted
            lastEnded=lastEnded
            runFrequency=runFrequency
            totalRuns=totalRuns
            success=successRuns
            failure=failureRuns
            exceptions=exceptions
            notes=notes
            currentState=currentState
          }
          defs{
            defaultIdleState="idle"
            defaultRunningState="running"
            defaultStatusVal="0"
            sentinelDate="0001-01-01 00:00:00"
            defaultDateFormat="yyyy-MM-dd HH:mm:ss"
            NoException="none"
            min=min
            hour=hour
            day=day
            week=week
            alpha="[^a-zA-Z]"
            excStrEnd=100
          }
        }
        kafkaRequests{
          name=kafkaRequests
          cols{
            request=request
            processDate=processDate
            cluster=cluster
            topic=topic
            alias=alias
            groupName=groupName
            exceptions=exceptions
            notes=notes
            currentState=currentState
          }
          defs{
            defaultIdleState="idle"
            defaultRunningState="running"
            defaultStatusVal="0"
            sentinelDate="0001-01-01 00:00:00"
            defaultDateFormat="yyyy-MM-dd HH:mm:ss"
            NoException="none"
            min=min
            hour=hour
            day=day
            week=week
            alpha="[^a-zA-Z]"
            excStrEnd=100
          }
        }
        bookmarks{
          name=BookMarks
          cols{
            dbtablekey=primarykey
            id=id
            bookmarkId=bookmark
          }
          defs{
            defaultDateBookMarkValue="0001-01-01 00:00:00"
            defaultIdBookMarkValue="0"
            defaultDateFormat="yyyy-MM-dd HH:mm:ss"
            running=RUNNING
          }
        }
        kafkabookmarks{
          name=kafkaBookMarks
          cols{
            id=id
            kafkatopickey=primarykey
            bookmark=bookmark
          }
          defs{
            defaultDateBookMarkValue="0001-01-01 00:00:00"
            defaultIdBookMarkValue="0"
            defaultDateFormat="yyyy-MM-dd HH:mm:ss"
            running=RUNNING
          }
        }
        statustmp{
          name=Status_tmp
        }
        status{
          name=Status
          cols{
            dbname=dbname
            dbtable=tablename
            primarykey=primarykey
            sourceId=sourceid
            kafkaTargetId=kafkaTargetid
            hdfsTargetId=hdfsTargetid
            qualifiedName=qualifiedName
            timestampSubstrIndex=10
          }
          defs{
            defaultTargetId="0001-01-01 00:00:00"
          }
        }
      }
    }
  }
  type= com.askme.ramanujan.server.RootServer
  port=7172
  host=127.0.0.1
  timeout=120
  actorSystem {
    name=${cluster.name}"-akka"
    akka {
      loggers = ["akka.event.slf4j.Slf4jLogger"]
      loglevel = INFO
    }
    actors {
      listener="ramanujan_pipeline_listener"
      master="ramanujan_pipeline_master"
      service="ramanujan_service_actor"
    }
    spray {
      can.server {
        server-header = ${component.name}
        remote-address-header=on
        request-timeout=120 s
        idle-timeout=600 s
        pipelining-limit = 5
        stats-support = off
        raw-request-uri-header = on
        parsing {
          max-uri-length = 10k
        }
      }
      routing {
        relaxed-header-parsing = on
      }
    }
  }
  spark {
    master="local[4]"
    app.name=${instance.fqn}
    spark.executor.memory="5g"
    spark.logConf="true"
    es.nodes = "mandelbrot01.staging.askmebazaar.com,mandelbrot02.staging.askmebazaar.com,mandelbrot03.staging.askmebazaar.com,mandelbrot04.staging.askmebazaar.com"
    es.port = 9200
    spark.serializer=org.apache.spark.serializer.KryoSerializer
    sun.io.serialization.extendedDebugInfo=true
  }
  handler {
    appname=Ramanujan
    baseurl=/ramanujan
    report=/report
    request=/request
    index.html=web/static/index.html
    timeout {
      scalar=120
      unit=seconds
    }
    name=${instance.fqn}"-http"
    timeoutms=120000
    stream {
      order-pipeline {
        source {
          topic = "orderservice_prod"
          brokers = "kafka01.production.askmebazaar.com:9092,kafka02.production.askmebazaar.com:9092,kafka03.production.askmebazaar.com:9092"
          #brokers = "dc1.staging.askme.com:9092,dc2.staging.askme.com:9092,dc3.staging.askme.com:9092,dc4.staging.askme.com:9092,dc5.staging.askme.com:9092"
        }
      }
    }
  }
}